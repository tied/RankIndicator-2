ProjectName;Key;title_desc;Type;Created;LastUpdatedStatus;LastUpdatedStatusDate;Priority;Storypoint;NumWatchers;NumComments;NumIssueLinks;NumAffectedVersions;NumFixVersions;NumChangeHistory;P;S;R
SERVER;SERVER-49203;jepsen smoke 15 system failure rate failures manifest like noformat 2020 06 03 202422 459 error 2020 06 03 202422 458 main jepsen cli oh jeez im sorry jepsen broke heres 2020 06 03 202422 459 java util concurrent executionexception java lang runtimeexception mongo setup ip 10 122 57 220000 timed 2020 06 03 202422 459 java util concurrent futuretask report futuretask java122 na1 8 0_252 2020 06 03 202422 459 java util concurrent futuretask get futuretask java192 na1 8 0_252 2020 06 03 202422 459 clojure corederef_future invokestatic core clj2208 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure corefuture_callreify__6962 deref core clj6688 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure corederef invokestatic core clj2228 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure corederef invoke core clj2214 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure coremapfn__4785 invoke core clj2644 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang lazyseq sval lazyseq java40 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang lazyseq seq lazyseq java49 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang rt seq rt java521 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure coreseq__4357 invokestatic core clj137 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure core protocolsseq_reduce invokestatic protocols clj24 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure core protocolsfn__6738 invokestatic protocols clj75 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure core protocolsfn__6738 invoke protocols clj75 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure core protocolsfn__6684g__6679__6697 invoke protocols clj13 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure corereduce invokestatic core clj6545 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure coreinto invokestatic core clj6610 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure coreinto invoke core clj6604 clojure 1 8 0 jarna 2020 06 03 202422 459 jepsen controlon_nodes invokestatic control clj373 jepsen 0 1 8 jarna 2020 06 03 202422 459 jepsen controlon_nodes invoke control clj357 jepsen 0 1 8 jarna 2020 06 03 202422 459 jepsen controlon_nodes invokestatic control clj362 jepsen 0 1 8 jarna 2020 06 03 202422 459 jepsen controlon_nodes invoke control clj357 jepsen 0 1 8 jarna 2020 06 03 202422 459 jepsen corerun_bang_fn__4284fn__4287 invoke core clj584 jepsen 0 1 8 jarna 2020 06 03 202422 459 jepsen corerun_bang_fn__4284 invoke core clj572 jepsen 0 1 8 jarna 2020 06 03 202422 459 jepsen corerun_bang_ invokestatic core clj553 jepsen 0 1 8 jarna 2020 06 03 202422 459 jepsen corerun_bang_ invoke core clj500 jepsen 0 1 8 jarna 2020 06 03 202422 459 jepsen clisingle_test_cmdfn__4984 invoke cli clj329 jepsen 0 1 8 jarna 2020 06 03 202422 459 jepsen clirun_bang_ invokestatic cli clj273 jepsen 0 1 8 jarna 2020 06 03 202422 459 jepsen clirun_bang_ invoke cli clj203 jepsen 0 1 8 jarna 2020 06 03 202422 459 jepsen mongodb runner_main invokestatic runner clj164 classes na 2020 06 03 202422 459 jepsen mongodb runner_main doinvoke runner clj162 classes na 2020 06 03 202422 459 clojure lang restfn invoke restfn java3894 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang var invoke var java676 clojure 1 8 0 jarna 2020 06 03 202422 459 usereval5 invokestatic form init3654414589850886228 clj1 nana 2020 06 03 202422 459 usereval5 invoke form init3654414589850886228 clj1 nana 2020 06 03 202422 459 clojure lang compiler eval compiler java6927 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang compiler eval compiler java6917 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang compiler load compiler java7379 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang compiler loadfile compiler java7317 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure mainload_script invokestatic main clj275 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure maininit_opt invokestatic main clj277 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure maininit_opt invoke main clj277 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure maininitialize invokestatic main clj308 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure mainnull_opt invokestatic main clj342 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure mainnull_opt invoke main clj339 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure mainmain invokestatic main clj421 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure mainmain doinvoke main clj384 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang restfn invoke restfn java421 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang var invoke var java383 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang afn applytohelper afn java156 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang var applyto var java700 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure main main main java37 clojure 1 8 0 jarna 2020 06 03 202422 459 caused java lang runtimeexception mongo setup ip 10 122 57 220000 timed 2020 06 03 202422 459 jepsen mongodb coredbreify__2125 setup_bang_ core clj330 classes na 2020 06 03 202422 459 jepsen dbcycle_bang_ invokestatic db clj25 jepsen 0 1 8 jarna 2020 06 03 202422 459 jepsen dbcycle_bang_ invoke db clj20 jepsen 0 1 8 jarna 2020 06 03 202422 459 clojure corepartialfn__4759 invoke core clj2516 clojure 1 8 0 jarna 2020 06 03 202422 459 jepsen controlon_nodesfn__2069 invoke control clj372 jepsen 0 1 8 jarna 2020 06 03 202422 459 clojure lang afn applytohelper afn java154 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang afn applyto afn java144 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure coreapply invokestatic core clj646 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure corewith_bindings_star_ invokestatic core clj1881 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure corewith_bindings_star_ doinvoke core clj1881 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang restfn applyto restfn java142 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure coreapply invokestatic core clj650 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure corebound_fn_star_fn__4671 doinvoke core clj1911 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang restfn invoke restfn java408 clojure 1 8 0 jarna 2020 06 03 202422 459 jepsen utilreal_pmaplauncher__1160fn__1161 invoke util clj49 jepsen 0 1 8 jarna 2020 06 03 202422 459 clojure corebinding_conveyor_fnfn__4676 invoke core clj1938 clojure 1 8 0 jarna 2020 06 03 202422 459 clojure lang afn call afn java18 clojure 1 8 0 jarna 2020 06 03 202422 459 java util concurrent futuretask run futuretask java266 na1 8 0_252 2020 06 03 202422 459 java util concurrent threadpoolexecutor runworker threadpoolexecutor java1149 na1 8 0_252 2020 06 03 202422 459 java util concurrent threadpoolexecutorworker run threadpoolexecutor java624 na1 8 0_252 2020 06 03 202422 459 java lang thread run thread java748 na1 8 0_252 noformat repeated execution latest failure https evergreen mongodb com task mongodb_mongo_master_ubuntu1804_debug_aubsan_lite_jepsen_smoke_e53293b8749c692ae2abe50ff02f4aee6fea8b84_20_06_30_10_26_41 green indicating transient error rather something linked server represents bug test infrastructure rather server fixes see are# disable task# add retry logic task# dig deeper jepsen test figure happens roughly 15 time;Bug;2020-06-30T20:03:44.000+0000;Open;2020-06-30T20:03:50.000+0000;Major - P3;0.0;3;1;0;1;1;12;1;0;4
SERVER;SERVER-50331;add genny workload maintenance events build variant add genny workload maintenance events build variant;Improvement;2020-08-17T10:12:23.000+0000;Blocked;2020-08-19T16:20:05.000+0000;Major - P3;0.0;1;0;0;0;1;9;1;0;4
SERVER;SERVER-50282;provide debugging setup script spawnhosts load artifacts coredumps overlong filenames truncating important properties actually bug ticket repurposed provide script unpackages files necessary inspecting coredump spawnhost assumes bug eventually fixed bug impacts subset cases original descriptionthe time spawn host data files test failure theres available core dump want load gdb script programmatically unpackages everything appropriate directory whether server engineers use script set gdb usage believe spawning host investigate core dump common use case unfortunately filenames long important properties trimmed https github com evergreen ci evergreen blob 86ebeb15ddc211f1390c5cc56af54a3712728e62 operations fetch go#l476 keyword coredump 1 makes difficult breaks script acceptable sort scripting isnt supported built established agreement also breaks ability corollary work hand tar tf archive afaik complete filescan point faster download coredumps hand arguably defeats purpose spawning host artifacts loaded dont know feasible solution theres probably reason filenames long uniqueness though imo unreadable ideas use shorter strings evergreen fetch generate preserve contents archive expense labeling variant task id afaik becomes problem user fetches artifacts multiple tasks directory backwords breaking established use cases consider adding flag fetch e g evergreen fetch task artifacts shortnames let users spawning host loading data opt short filenames add environment variables containing absolute paths interesting artifacts users sshing instance scripts hook without needing rely filename patterns e g bin_archive archive containing mongod dbg_archive archive containing mongod debug coredump_archive archive containing coredumps src_dir mongodb repository path fetch sources 1 noformat root ip 10 122 8 102 # data mci artifacts patch 1419_linux 64 debug_ data mci artifacts patch 1419_linux 64 debug_compiletotal 2503040 rw r r 1 root root 136935 may 19 0145 config mongodb_mongo_v4 4_linux_64_debug_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 log rw r r 1 root root 2473743732 may 19 0146 debugsymbols mongodb_mongo_v4 4_linux_64_debug_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 tgz rw r r 1 root root 84980170 may 19 0145 mongo mongodb_mongo_v4 4_linux_64_debug_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 tgz rw r r 1 root root 3536789 may 19 0145 mongodb_mongo_v4 4_linux_64_debug_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 tgz rw r r 1 root root 1097 may 19 0145 pip requirements mongodb_mongo_v4 4_linux_64_debug_compile_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 0 txt rw r r 1 root root 699562 may 19 0145 scons cache mongodb_mongo_v4 4_linux_64_debug_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 0 log data mci artifacts patch 1419_linux 64 debug_jscoretotal 120 rw r r 1 root root 80088 may 19 0145 running tests evergreen tasks locally rw r r 1 root root 1446 may 19 0145 mongo diskstats mongodb_mongo_v4 4_linux_64_debug_jscore_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 0 tgz rw r r 1 root root 29980 may 19 0145 mongo system resource info mongodb_mongo_v4 4_linux_64_debug_jscore_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 0 tgz rw r r 1 root root 1097 may 19 0145 pip requirements mongodb_mongo_v4 4_linux_64_debug_jscore_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 0 txt data mci artifacts patch 1419_linux 64 debug_retryable_writes_jscore_stepdown_passthroughtotal 1204712 rw r r 1 root root 80088 may 19 0145 running tests evergreen tasks locally rw r r 1 root root 1212594087 may 19 0146 4_linux_64_debug_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 retryable_writes_jscore_stepdown_passthrough 0 tgz rw r r 1 root root 10900 may 19 0145 4_linux_64_debug_retryable_writes_jscore_stepdown_passthrough_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 0 tgz rw r r 1 root root 20560324 may 19 0145 m_ 1 4_linux_64_debug_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 retryable_writes_jscore_stepdown_passthrough 0 tgz rw r r 1 root root 262274 may 19 0145 m_ 1 4_linux_64_debug_retryable_writes_jscore_stepdown_passthrough_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 0 tgz rw r r 1 root root 101019 may 19 0145 m_ 2 4_linux_64_debug_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 retryable_writes_jscore_stepdown_passthrough 0 tgz rw r r 1 root root 1097 may 19 0145 p 4_linux_64_debug_retryable_writes_jscore_stepdown_passthrough_patch_1d5d11155689d29bb7de42ccb5a5f4b3c7247469_5ebf0cd932f4170aad0ca35f_20_05_15_21_43_14 0 txt noformat;Improvement;2020-05-19T19:29:00.000+0000;Closed;2020-08-20T15:58:06.000+0000;Major - P3;0.0;6;5;5;0;1;75;1;0;4
SERVER;SERVER-38292;mongodb crash got signal 11 segmentation fault also issue 3 2 19 3 4 14 wt engine heres stack 3 4 14 logs brief exactly fix wt 4037 clear ticket codejava 2018 11 28t100613 885 0600 f thread2 invalid access address 02018 11 28t100613 906 0600 f thread2 got signal 11 segmentation fault 0x562636fa4a51 0x562636fa3c69 0x562636fa42d6 0x7f74ee9647e0 0x56263716bca3 0x56263716bd7c 0x562637aa1d7a 0x5626378e0466 0x5626378e05ea 0x5626378e0c08 0x5626378e3ac3 0x562637934294 0x562637934e47 0x56263792f813 0x56263792fba7 0x5626379316a3 0x56263799b906 0x7f74ee95caa1 0x7f74ee6a9c4d begin backtrace backtrace b562635a2b000 o1579a51 s_zn5mongo15printstacktraceerso b562635a2b000 o1578c69 b562635a2b000 o15792d6 b7f74ee955000 of7e0 b562635a2b000 o1740ca3 s_zn8tcmalloc11threadcache21releasetocentralcacheepns0_8freelistemi b562635a2b000 o1740d7c s_zn8tcmalloc11threadcache11listtoolongepns0_8freelistem b562635a2b000 o2076d7a s_zdlpvrkst9nothrow_t b562635a2b000 o1eb5466 s__wt_split_stash_discard b562635a2b000 o1eb55ea b562635a2b000 o1eb5c08 b562635a2b000 o1eb8ac3 s__wt_split_reverse b562635a2b000 o1f09294 b562635a2b000 o1f09e47 s__wt_evict b562635a2b000 o1f04813 b562635a2b000 o1f04ba7 b562635a2b000 o1f066a3 s__wt_evict_thread_run b562635a2b000 o1f70906 s__wt_thread_run b7f74ee955000 o7aa1 b7f74ee5c1000 oe8c4d sclone processinfo mongodbversion 3 4 14 gitversion fd954412dfc10e4d1e3e2dd4fac040f8b476b268 compiledmodules uname sysname linux release 2 6 32 754 6 3 el6 x86_64 version #1 smp tue oct 9 172749 utc 2018 machine x86_64 somap b 562635a2b000 elftype 3 buildid 64cd384c41acc8d81741f0dfc0f9a3d7756f81ff b 7fff3e8f6000 elftype 3 buildid f9f48cc73d4d61ae273899b31855c6589ee5ea8d b 7f74ef7fd000 path usr lib64 libssl 10 elftype 3 buildid becfb85a8bc084042d5bf2ba9e66325ce798b659 b 7f74ef418000 path usr lib64 libcrypto 10 elftype 3 buildid cbda444a7109874c5350ae9ceef3f82f749b347f b 7f74ef210000 path lib64 librt 1 elftype 3 buildid 552cec3216281ccfd7fa6432c723d50163255823 b 7f74ef00c000 path lib64 libdl 2 elftype 3 buildid 2af795bffd122309ba3359febabb5d0967403d17 b 7f74eed88000 path lib64 libm 6 elftype 3 buildid 4aaee970b045d8bf946578b9c7f3ab5cde9ab44a b 7f74eeb72000 path lib64 libgcc_s 1 elftype 3 buildid edc925e58fe28dca536993eb13179c739f1e6566 b 7f74ee955000 path lib64 libpthread 0 elftype 3 buildid 4ea475cd3fd3b69b6c95d9381fa74b36db4992ef b 7f74ee5c1000 path lib64 libc 6 elftype 3 buildid bca7789c2ea8e28cb7ce553e183ac7e7ee36f8a2 b 7f74efa69000 path lib64 ld linux x86 64 2 elftype 3 buildid 97af4b77212f74cff72b6c013e6aa2d74a97ef60 b 7f74ee37d000 path lib64 libgssapi_krb5 2 elftype 3 buildid 9a737f8bf10fc99c37cc404d3fc188f6e11fedd9 b 7f74ee096000 path lib64 libkrb5 3 elftype 3 buildid 8d3d6e28df6eb3752642a7031aac17d39ea4265d b 7f74ede92000 path lib64 libcom_err 2 elftype 3 buildid 7ec54d6e88bb7d2c1284117c2a483496a01eaaf4 b 7f74edc66000 path lib64 libk5crypto 3 elftype 3 buildid cc89b4c8cdccd32ba610bc72784dc3b7e9bd9e19 b 7f74eda50000 path lib64 libz 1 elftype 3 buildid 5fa8e5038ec04a774af72a9bb62dc86e1049c4d6 b 7f74ed845000 path lib64 libkrb5support 0 elftype 3 buildid e0c522c589f775c324330be09ce67dc83950a213 b 7f74ed642000 path lib64 libkeyutils 1 elftype 3 buildid af374bafb7f5b139a0b431d3f06d82014aff3251 b 7f74ed428000 path lib64 libresolv 2 elftype 3 buildid 4786a2a5d30b121601958e84d643c70c13c4fba5 b 7f74ed209000 path lib64 libselinux 1 elftype 3 buildid b4576be308ddcf7bc31f7304e4734c3d846d0236 mongod 3 4 _zn5mongo15printstacktraceerso+0x41 0x562636fa4a51 mongod 3 4 +0x1578c69 0x562636fa3c69 mongod 3 4 +0x15792d6 0x562636fa42d6 libpthread 0 +0xf7e0 0x7f74ee9647e0 mongod 3 4 _zn8tcmalloc11threadcache21releasetocentralcacheepns0_8freelistemi+0xe3 0x56263716bca3 mongod 3 4 _zn8tcmalloc11threadcache11listtoolongepns0_8freelistem+0x1c 0x56263716bd7c mongod 3 4 _zdlpvrkst9nothrow_t+0x26a 0x562637aa1d7a mongod 3 4 __wt_split_stash_discard+0xc6 0x5626378e0466 mongod 3 4 +0x1eb55ea 0x5626378e05ea mongod 3 4 +0x1eb5c08 0x5626378e0c08 mongod 3 4 __wt_split_reverse+0x83 0x5626378e3ac3 mongod 3 4 +0x1f09294 0x562637934294 mongod 3 4 __wt_evict+0xac7 0x562637934e47 mongod 3 4 +0x1f04813 0x56263792f813 mongod 3 4 +0x1f04ba7 0x56263792fba7 mongod 3 4 __wt_evict_thread_run+0xd3 0x5626379316a3 mongod 3 4 __wt_thread_run+0x16 0x56263799b906 libpthread 0 +0x7aa1 0x7f74ee95caa1 libc 6 clone+0x6d 0x7f74ee6a9c4d end backtrace code please note process generating core dump attach mongod process crashes;Bug;2018-11-28T17:25:46.000+0000;Closed;2019-02-19T01:00:03.000+0000;Major - P3;0.0;14;44;3;0;0;63;1;0;4
SERVER;SERVER-38292;mongodb crash got signal 11 segmentation fault also issue 3 2 19 3 4 14 wt engine heres stack 3 4 14 logs brief exactly fix wt 4037 clear ticket codejava 2018 11 28t100613 885 0600 f thread2 invalid access address 02018 11 28t100613 906 0600 f thread2 got signal 11 segmentation fault 0x562636fa4a51 0x562636fa3c69 0x562636fa42d6 0x7f74ee9647e0 0x56263716bca3 0x56263716bd7c 0x562637aa1d7a 0x5626378e0466 0x5626378e05ea 0x5626378e0c08 0x5626378e3ac3 0x562637934294 0x562637934e47 0x56263792f813 0x56263792fba7 0x5626379316a3 0x56263799b906 0x7f74ee95caa1 0x7f74ee6a9c4d begin backtrace backtrace b562635a2b000 o1579a51 s_zn5mongo15printstacktraceerso b562635a2b000 o1578c69 b562635a2b000 o15792d6 b7f74ee955000 of7e0 b562635a2b000 o1740ca3 s_zn8tcmalloc11threadcache21releasetocentralcacheepns0_8freelistemi b562635a2b000 o1740d7c s_zn8tcmalloc11threadcache11listtoolongepns0_8freelistem b562635a2b000 o2076d7a s_zdlpvrkst9nothrow_t b562635a2b000 o1eb5466 s__wt_split_stash_discard b562635a2b000 o1eb55ea b562635a2b000 o1eb5c08 b562635a2b000 o1eb8ac3 s__wt_split_reverse b562635a2b000 o1f09294 b562635a2b000 o1f09e47 s__wt_evict b562635a2b000 o1f04813 b562635a2b000 o1f04ba7 b562635a2b000 o1f066a3 s__wt_evict_thread_run b562635a2b000 o1f70906 s__wt_thread_run b7f74ee955000 o7aa1 b7f74ee5c1000 oe8c4d sclone processinfo mongodbversion 3 4 14 gitversion fd954412dfc10e4d1e3e2dd4fac040f8b476b268 compiledmodules uname sysname linux release 2 6 32 754 6 3 el6 x86_64 version #1 smp tue oct 9 172749 utc 2018 machine x86_64 somap b 562635a2b000 elftype 3 buildid 64cd384c41acc8d81741f0dfc0f9a3d7756f81ff b 7fff3e8f6000 elftype 3 buildid f9f48cc73d4d61ae273899b31855c6589ee5ea8d b 7f74ef7fd000 path usr lib64 libssl 10 elftype 3 buildid becfb85a8bc084042d5bf2ba9e66325ce798b659 b 7f74ef418000 path usr lib64 libcrypto 10 elftype 3 buildid cbda444a7109874c5350ae9ceef3f82f749b347f b 7f74ef210000 path lib64 librt 1 elftype 3 buildid 552cec3216281ccfd7fa6432c723d50163255823 b 7f74ef00c000 path lib64 libdl 2 elftype 3 buildid 2af795bffd122309ba3359febabb5d0967403d17 b 7f74eed88000 path lib64 libm 6 elftype 3 buildid 4aaee970b045d8bf946578b9c7f3ab5cde9ab44a b 7f74eeb72000 path lib64 libgcc_s 1 elftype 3 buildid edc925e58fe28dca536993eb13179c739f1e6566 b 7f74ee955000 path lib64 libpthread 0 elftype 3 buildid 4ea475cd3fd3b69b6c95d9381fa74b36db4992ef b 7f74ee5c1000 path lib64 libc 6 elftype 3 buildid bca7789c2ea8e28cb7ce553e183ac7e7ee36f8a2 b 7f74efa69000 path lib64 ld linux x86 64 2 elftype 3 buildid 97af4b77212f74cff72b6c013e6aa2d74a97ef60 b 7f74ee37d000 path lib64 libgssapi_krb5 2 elftype 3 buildid 9a737f8bf10fc99c37cc404d3fc188f6e11fedd9 b 7f74ee096000 path lib64 libkrb5 3 elftype 3 buildid 8d3d6e28df6eb3752642a7031aac17d39ea4265d b 7f74ede92000 path lib64 libcom_err 2 elftype 3 buildid 7ec54d6e88bb7d2c1284117c2a483496a01eaaf4 b 7f74edc66000 path lib64 libk5crypto 3 elftype 3 buildid cc89b4c8cdccd32ba610bc72784dc3b7e9bd9e19 b 7f74eda50000 path lib64 libz 1 elftype 3 buildid 5fa8e5038ec04a774af72a9bb62dc86e1049c4d6 b 7f74ed845000 path lib64 libkrb5support 0 elftype 3 buildid e0c522c589f775c324330be09ce67dc83950a213 b 7f74ed642000 path lib64 libkeyutils 1 elftype 3 buildid af374bafb7f5b139a0b431d3f06d82014aff3251 b 7f74ed428000 path lib64 libresolv 2 elftype 3 buildid 4786a2a5d30b121601958e84d643c70c13c4fba5 b 7f74ed209000 path lib64 libselinux 1 elftype 3 buildid b4576be308ddcf7bc31f7304e4734c3d846d0236 mongod 3 4 _zn5mongo15printstacktraceerso+0x41 0x562636fa4a51 mongod 3 4 +0x1578c69 0x562636fa3c69 mongod 3 4 +0x15792d6 0x562636fa42d6 libpthread 0 +0xf7e0 0x7f74ee9647e0 mongod 3 4 _zn8tcmalloc11threadcache21releasetocentralcacheepns0_8freelistemi+0xe3 0x56263716bca3 mongod 3 4 _zn8tcmalloc11threadcache11listtoolongepns0_8freelistem+0x1c 0x56263716bd7c mongod 3 4 _zdlpvrkst9nothrow_t+0x26a 0x562637aa1d7a mongod 3 4 __wt_split_stash_discard+0xc6 0x5626378e0466 mongod 3 4 +0x1eb55ea 0x5626378e05ea mongod 3 4 +0x1eb5c08 0x5626378e0c08 mongod 3 4 __wt_split_reverse+0x83 0x5626378e3ac3 mongod 3 4 +0x1f09294 0x562637934294 mongod 3 4 __wt_evict+0xac7 0x562637934e47 mongod 3 4 +0x1f04813 0x56263792f813 mongod 3 4 +0x1f04ba7 0x56263792fba7 mongod 3 4 __wt_evict_thread_run+0xd3 0x5626379316a3 mongod 3 4 __wt_thread_run+0x16 0x56263799b906 libpthread 0 +0x7aa1 0x7f74ee95caa1 libc 6 clone+0x6d 0x7f74ee6a9c4d end backtrace code please note process generating core dump attach mongod process crashes;Bug;2018-11-28T17:25:46.000+0000;Closed;2019-06-03T22:05:06.000+0000;Major - P3;0.0;14;44;3;0;0;63;1;0;4
SERVER;SERVER-37866;add performance testing verify replication lag throttled desired testing ensure lag appropriately throttled also election happens new primary chosen may want test also verify effects clock skew flow control mechanism;Task;2018-11-01T15:47:56.000+0000;Open;2018-11-01T15:48:01.000+0000;Major - P3;0.0;2;0;1;0;1;89;1;0;4
SERVER;SERVER-35714;recover common point rollback instead stable timestamp allow pit reads secondaries within previous batches able simply recover common point rollback instead recovering stable timestamp replaying oplog common point make rollback even faster easier understand recovery code used startup recovery;Improvement;2018-06-20T21:22:30.000+0000;Open;2019-10-04T04:57:34.000+0000;Major - P3;0.0;4;2;1;0;1;20;1;0;4
SERVER;SERVER-20960;default index build option support config runtime add support index build preferences config file mongod runtime parameters specifically control preferences background foreground index build options edit server 24041 following request added quote would great block foreground index creation configuration quote;New Feature;2015-10-16T03:14:25.000+0000;Closed;2019-01-17T21:12:54.000+0000;Minor - P4;0.0;27;4;4;7;0;56;0;0;7
SERVER;SERVER-49164;sweep fix missing dependencies evergreen yml noticed tasks like jscore txns large txns format dependent compile probably makes sense depend jscorehttps evergreen mongodb com task mongodb_mongo_master_enterprise_rhel_62_64_bit_dynamic_required_jscore_txns_large_txns_format_3ea60282c8841c67d4e2f3a365b7f1640c84198c_20_06_29_14_07_33while seems like someone one time sweep see theres dependencies would make sense add know doesnt fit perfectly stm charter tbh probably doesnt fit well anyones charter seems like cleanest fitcc pasette;Improvement;2020-06-29T15:05:45.000+0000;In Code Review;2020-08-20T15:03:57.000+0000;Major - P3;0.0;5;5;0;0;1;9;1;0;4
SERVER;SERVER-50397;move jepsen smoke test separate builder jepsen smoke test currently runs aubsan builder consistently purple due timing issues even though real jepsen tasks green move separate builder avoid stability issues;Task;2020-08-20T12:39:51.000+0000;Open;2020-08-20T12:39:55.000+0000;Major - P3;1.0;1;0;0;0;1;4;1;0;4
SERVER;SERVER-50379;reduce frequency builders 4 4 4 4 released wed like adjust frequency hourly builders run higher interval;Task;2020-08-19T12:57:28.000+0000;Open;2020-08-19T12:57:32.000+0000;Major - P3;1.0;0;0;0;0;1;3;1;0;4
SERVER;SERVER-50255;unittest task failures evergreen dont capture shared library files lack shared library files prevents core dump captured usable heres patch build demonstrates problem https spruce mongodb com task mongodb_mongo_master_linux_64_debug_unittests_patch_b9d6ae55fd793711dd539fa9322ab5f916b4c89a_5f31ae7357e85a0c7c306166_20_08_10_20_31_46 files;Bug;2020-08-11T17:17:50.000+0000;In Code Review;2020-08-18T14:33:56.000+0000;Major - P3;1.0;6;4;0;0;1;7;1;0;4
SERVER;SERVER-49764;update instructions running genny sys perf patch builds instructions system_perf yml genny patch tasks mentions force workloads longer option genny update instructions new approach may also consider updating genny reject unknown arguments;Bug;2020-07-21T15:41:09.000+0000;Open;2020-07-21T15:41:10.000+0000;Major - P3;1.0;0;0;0;0;1;4;1;0;4
SERVER;SERVER-32891;sys perf change order mongodb_setup workload_setup calls longevity also;Task;2018-01-25T14:46:49.000+0000;Closed;2018-01-31T14:54:44.000+0000;Major - P3;1.0;1;4;0;0;4;16;1;0;4
SERVER;SERVER-33134;add ycsb retryable writes enabled tasks system_perf yml needed perf 1129 https jira mongodb org browse perf 1129;Task;2018-02-05T21:15:39.000+0000;Open;2018-02-05T21:47:02.000+0000;Major - P3;1.0;2;0;0;0;1;28;1;0;4
SERVER;SERVER-48113;add test cwd resmoke tests make dumb sanity test asserts cwd mongo repo root perhaps also import key modules detect user pip install ing right things goal help users use ides assume cwd buildscripts directory something else otherwise results cryptic error messages ensure first test run naming file something like test_aaa_sanitycheck;Improvement;2020-05-11T21:05:15.000+0000;Open;2020-05-11T21:05:16.000+0000;Major - P3;1.0;0;0;0;0;1;21;1;0;4
SERVER;SERVER-48147;remove fallbacklogger current buildlogger implementation fallback logger logs stderr https github com mongodb mongo blob e2602ad053b2120982fbcac8e33e1ad64e6ec30a buildscripts resmokelib logging loggers py#l352 cases buildlogger failing https github com mongodb mongo blob e2602ad053b2120982fbcac8e33e1ad64e6ec30a buildscripts resmokelib logging buildlogger py#l53 fallback logger longer needed use jasper logging removed;Task;2020-05-12T17:37:33.000+0000;Open;2020-05-19T15:11:44.000+0000;Major - P3;1.0;1;0;0;0;1;76;1;0;4
SERVER;SERVER-40872;spawning host data failed task longer includes debug symbols clicking spawn link evergreen leaving load data onto host startup checkbox checked would cause evergreen run evergreen fetch command host provisioned evergreen fetch command would download artifacts attached task using s3 put attach artifacts commands well artifacts attached tasks depends e g compile task uploading debug symbols moved separate upload_debug_symbols task tasks depend upload_debug_symbols task tarball containing debug symbols downloaded evergreen fetch command run attach artifacts command part compile task preserve old behavior noformat name mongo debugsymbols tgz link https s3 amazonaws com mciuploads mongo_debugsymbols visibility public noformat separately remove ignore_artifacts_for_spawn thats specified attach artifacts command etc evergreen yml project configuration file isnt actually used evergreen whether artifacts downloaded spawn host controlled ignore_for_fetch parameter within file description https github com evergreen ci evergreen wiki project commands#attach artifacts;Bug;2019-04-27T02:50:57.000+0000;Open;2019-04-27T02:51:05.000+0000;Major - P3;1.0;2;3;2;0;1;15;1;0;4
SERVER;SERVER-48961;add expansion powercycle user evergreen yml add expansion powercycle build variants sets username administrator windows evergreen user otherwise wrapper script created server 48960 use expansion value calling remoteoperations;Bug;2020-06-18T15:55:26.000+0000;Open;2020-07-10T14:46:15.000+0000;Major - P3;1.0;1;0;2;0;1;19;1;0;4
SERVER;SERVER-48961;add expansion powercycle user evergreen yml add expansion powercycle build variants sets username administrator windows evergreen user otherwise wrapper script created server 48960 use expansion value calling remoteoperations;Bug;2020-06-18T15:55:26.000+0000;Open;2020-06-18T15:55:27.000+0000;Major - P3;1.0;1;0;2;0;1;19;1;0;4
SERVER;SERVER-49097;sys perf builds differ release builds sys perf artifacts multiple gigs whereas regular waterfall builds dozens megs cursory glance shows may debug build sys perf;Bug;2020-06-25T14:30:26.000+0000;Open;2020-06-25T14:30:27.000+0000;Major - P3;0.5;3;5;0;1;2;16;1;0;4
SERVER;SERVER-49199;selected_tests_gen run patch builds noop waterfall currently selected_tests_gen runs waterfall every 7 days https github com mongodb mongo blob fdd66b8c2c0f5c818a01ecd64a27b0b7c0ca28d6 etc evergreen yml#l12736 resulting occasional bfs https jira mongodb org browse bf 17966 since selected_tests_gen task really useful making patch builds efficient reason run waterfall server engineer selected_tests_gen run waterfall takes unnecessary resources provide value ac selected_tests_gen run every 7 days waterfall noops rather runs selected_tests py script;Improvement;2020-06-30T19:10:37.000+0000;Open;2020-07-31T14:47:39.000+0000;Major - P3;1.0;0;0;0;0;0;6;1;0;4
SERVER;SERVER-48455;migrate oom tracker logic evergreen logic https github com mongodb mongo blob 21ef00aa502d7b74e19bc3b6b76f285c9860da3e etc evergreen yml#l3248 track evergreen tasks failed oom kills incorporated evergreen https github com evergreen ci evergreen commit ff076a5811c0ce205988ffa16f0e3c1fad3fb857 evergreens version enabled checks removed tasks pre post probably interim period run validate evergreens version;Improvement;2020-05-27T20:41:48.000+0000;Needs Verification;2020-07-15T22:37:04.000+0000;Major - P3;1.0;2;3;5;0;0;58;1;0;4
SERVER;SERVER-33134;add ycsb retryable writes enabled tasks system_perf yml needed perf 1129 https jira mongodb org browse perf 1129;Task;2018-02-05T21:15:39.000+0000;Open;2018-05-29T17:37:31.000+0000;Major - P3;1.0;2;0;0;0;1;28;1;0;4
SERVER;SERVER-19895;resmoke failures self document resmoke fails print steps help user debug failure e g resmoke detects run evergreen print places user look symptoms original descriptionat shutdown time fixture replication sharding checks return status process shutdown procedures returned false means process returned non 0 exit status fails test suite associated fixture would helpful fixture wrote message log stating process caused suite failure currently way diagnose scour logs looking exit status process since looking line exited code 0 simple search undertake;Improvement;2015-08-12T17:58:44.000+0000;Closed;2019-11-22T20:19:12.000+0000;Major - P3;2.0;7;14;0;0;1;58;1;1;5
SERVER;SERVER-33647;enterprise windows 2008r2 wiredtiger develop shouldnt run push task originally thought setting push_path push_bucket push_name push_arch enterprise windows 2008r2 wiredtiger develop builders expansions would sufficient make push evergreen task op however learned go yaml yaml known behavior differences merge key implemented see go yaml yaml#81 https github com go yaml yaml issues 81 go yaml yaml#325 https github com go yaml yaml issues 325 cause inherit enterprise windows 2008r2 builders configuration https github com mongodb mongo blob 4c97ba23c51ca5b8a5bcb3ea09f8292773aedd3d etc evergreen yml#l5909 l5912 via enterprise windows 64 2k8 template anchor means push task runs enterprise windows 2008r2 wiredtiger develop builder binaries uploaded s3 path releases mongodb happen via mongo release evergreen project issue impacts contents latest nightly releases;Task;2018-03-03T00:53:46.000+0000;Open;2019-06-11T20:04:22.000+0000;Major - P3;2.0;5;11;4;0;1;57;1;1;5
SERVER;SERVER-33647;enterprise windows 2008r2 wiredtiger develop shouldnt run push task originally thought setting push_path push_bucket push_name push_arch enterprise windows 2008r2 wiredtiger develop builders expansions would sufficient make push evergreen task op however learned go yaml yaml known behavior differences merge key implemented see go yaml yaml#81 https github com go yaml yaml issues 81 go yaml yaml#325 https github com go yaml yaml issues 325 cause inherit enterprise windows 2008r2 builders configuration https github com mongodb mongo blob 4c97ba23c51ca5b8a5bcb3ea09f8292773aedd3d etc evergreen yml#l5909 l5912 via enterprise windows 64 2k8 template anchor means push task runs enterprise windows 2008r2 wiredtiger develop builder binaries uploaded s3 path releases mongodb happen via mongo release evergreen project issue impacts contents latest nightly releases;Task;2018-03-03T00:53:46.000+0000;Open;2018-03-30T12:18:55.000+0000;Major - P3;2.0;5;11;4;0;1;57;1;1;5
SERVER;SERVER-33647;enterprise windows 2008r2 wiredtiger develop shouldnt run push task originally thought setting push_path push_bucket push_name push_arch enterprise windows 2008r2 wiredtiger develop builders expansions would sufficient make push evergreen task op however learned go yaml yaml known behavior differences merge key implemented see go yaml yaml#81 https github com go yaml yaml issues 81 go yaml yaml#325 https github com go yaml yaml issues 325 cause inherit enterprise windows 2008r2 builders configuration https github com mongodb mongo blob 4c97ba23c51ca5b8a5bcb3ea09f8292773aedd3d etc evergreen yml#l5909 l5912 via enterprise windows 64 2k8 template anchor means push task runs enterprise windows 2008r2 wiredtiger develop builder binaries uploaded s3 path releases mongodb happen via mongo release evergreen project issue impacts contents latest nightly releases;Task;2018-03-03T00:53:46.000+0000;Open;2018-03-12T06:12:04.000+0000;Major - P3;2.0;5;11;4;0;1;57;1;1;5
SERVER;SERVER-33972;bind jstestfuzz version server version fuzzer becomes dependent code server repo like rollback fixture impending preamble hooks would nice make change server wait change fuzzer wait bit go back server remove code left place support old fuzzer behavior able making jstestfuzz module evergreen yml;Improvement;2018-03-19T14:18:44.000+0000;Open;2018-04-04T14:00:40.000+0000;Major - P3;2.0;2;3;0;0;1;16;1;1;5
SERVER;SERVER-36816;avoid reloading view catalog primary secondaries dbhash check changes server 25640 made listcollections command run query containing names collections returned dbhash command primary https github com mongodb mongo blob 2bed54b084995f2c2dd048b6a70b6fd678e1ac30 src mongo shell replsettest js#l1855 l1858 secondary https github com mongodb mongo blob 2bed54b084995f2c2dd048b6a70b6fd678e1ac30 src mongo shell replsettest js#l1907 l1910 query leads view catalog reloaded special filter must used https github com mongodb mongo blob 2bed54b084995f2c2dd048b6a70b6fd678e1ac30 src mongo db commands list_collections_filter cpp#l37 l42 prevent behavior logic checkdbhashesforreplset function thats enabled fuzzer test suites skip checking dbhash reloading view catalog fails due invalid view definition https github com mongodb mongo blob 2bed54b084995f2c2dd048b6a70b6fd678e1ac30 src mongo shell replsettest js#l1860 l1865 however seems worthwhile avoid reloading view catalog weve found invalidnamespace error response may returned certain patterns involving null bytes instead use special filter prevent view catalog reloaded server listcollections command actual filtering client side codejavascript dont run validate view namespaces let filter type collection jstest options skipvalidationoninvalidviewdefinitions skipvalidationoninvalidviewdefinitionstrue avoid resolving view catalog admin database todo server 25493 remove exists clause performing initial sync versions mongodb 3 2 longer supported filter filter type exists false code;Improvement;2018-08-23T03:40:05.000+0000;Closed;2019-08-22T13:46:35.000+0000;Critical - P2;2.0;6;5;3;0;2;54;2;1;2
SERVER;SERVER-37052;add missing check replicasetmonitor error js retryable writes override replica set monitor throw two types errors 1 https github com mongodb mongo blob 8256436a338da3a9c984f2400fce142415633206 src mongo client dbclient_rs cpp#l283 2 https github com mongodb mongo blob 8256436a338da3a9c984f2400fce142415633206 src mongo client dbclient_rs cpp#l320 master visible retryable writes js override check types errors https github com mongodb mongo blob 8256436a338da3a9c984f2400fce142415633206 jstests libs override_methods auto_retry_on_network_error js#l338 l339;Bug;2018-09-07T18:35:10.000+0000;Open;2018-09-07T18:35:14.000+0000;Major - P3;2.0;1;1;0;0;1;18;1;1;5
SERVER;SERVER-37285;enterprise packages conflict non enterprise packages enterprise packages conflict non enterprise versions causes package managers like dpkg fail unhelpful error messages see attached screenshot image 2018 09 24 10 59 51 374 png thumbnail;Bug;2018-09-24T15:00:21.000+0000;Open;2018-09-25T13:53:09.000+0000;Major - P3;2.0;5;0;0;0;1;20;1;1;5
SERVER;SERVER-33647;enterprise windows 2008r2 wiredtiger develop shouldnt run push task originally thought setting push_path push_bucket push_name push_arch enterprise windows 2008r2 wiredtiger develop builders expansions would sufficient make push evergreen task op however learned go yaml yaml known behavior differences merge key implemented see go yaml yaml#81 https github com go yaml yaml issues 81 go yaml yaml#325 https github com go yaml yaml issues 325 cause inherit enterprise windows 2008r2 builders configuration https github com mongodb mongo blob 4c97ba23c51ca5b8a5bcb3ea09f8292773aedd3d etc evergreen yml#l5909 l5912 via enterprise windows 64 2k8 template anchor means push task runs enterprise windows 2008r2 wiredtiger develop builder binaries uploaded s3 path releases mongodb happen via mongo release evergreen project issue impacts contents latest nightly releases;Task;2018-03-03T00:53:46.000+0000;Open;2018-05-10T02:39:52.000+0000;Major - P3;2.0;5;11;4;0;1;57;1;1;5
SERVER;SERVER-47548;fix remove unittests_ yml test suites unittests_ yml test suites added server 18839 make easier run c++ unit tests related particular component locally however longer meaningfully run unit tests executables collapsed single executable per directory part changes server 41809 doubly unable meaningfully run result switch hygienic builds due build unittests txt refers files build install bin directory;Task;2020-04-15T01:06:42.000+0000;Open;2020-05-07T15:37:42.000+0000;Major - P3;2.0;2;1;1;0;1;5;1;1;5
SERVER;SERVER-47620;upgrade eslint eslint version outdated doesnt support newer js syntax like object spread shell supports;Improvement;2020-04-17T12:42:49.000+0000;Open;2020-05-07T15:37:58.000+0000;Major - P3;2.0;2;0;0;1;1;4;1;1;5
SERVER;SERVER-47882;update signal processing package version dag engineer id like use latest version signal processing newest version signal processing algorithms used ac upgrade 3 0 ensure perf sys perf updated backport;New Feature;2020-05-01T15:13:43.000+0000;Open;2020-05-08T14:29:27.000+0000;Major - P3;2.0;1;1;0;0;1;13;1;1;5
SERVER;SERVER-48110;simplify resmoke self checks make separate resmoke_selftest resmoke suite calls tests resmokelib fix ignore test_multi_js_test_selector_one_group since takes dozens seconds run remove exclude_files buildscripts_test resmoke suite instead unittest skip add single command script something runs relevant checks resmoke code checks including resmoke self tests linters write quick resmoke_readme md file inside resmokelib details setup test resmoke;Improvement;2020-05-11T21:02:41.000+0000;Open;2020-05-11T21:02:42.000+0000;Major - P3;2.0;0;0;0;0;1;8;1;1;5
SERVER;SERVER-48156;ensure resmoke works jasper jasper_process py added resmoke ago currently used ensure code still works server tests resmoke self tests succeed running jasper assume always logging stdout;Task;2020-05-12T18:54:20.000+0000;Blocked;2020-06-05T14:16:08.000+0000;Major - P3;2.0;0;5;0;1;1;74;1;1;5
SERVER;SERVER-48158;add resmoke testing jaspers logging endpoint part test driven development approach taking jasper resmoke integration project first add unittests resmoke following scenarios1 resmoke logging parent end point directly one resmoke one jasper process logging child endpoint2 two parent endpoints one child logging endpoint parent wed like assert output log exhibits properties logging hierarchy log go file memory buffer;Task;2020-05-12T18:58:09.000+0000;Closed;2020-07-23T15:57:53.000+0000;Major - P3;2.0;0;1;1;0;1;70;1;1;5
SERVER;SERVER-49504;allow resmokes mongo shell log jasper allow mongo shell spawned resmoke https github com mongodb mongo blob 316408d14e2358225e8f26bfe4d4022cfcf748d9 buildscripts resmokelib core programs py#l462 log logkeeper v3 suites dont use fixtures run almost entirely shell subprocesses making feature easy way validate new logging path well need set logkeepver v3 related credentials resmoke evergreen yml part ticket well;Task;2020-07-14T21:03:28.000+0000;Open;2020-07-28T20:36:45.000+0000;Major - P3;2.0;1;0;0;0;1;17;1;1;5
SERVER;SERVER-50000;run mongod mongos live record undodb enabled let live record execing way guarantee live record attached shared libs loaded;Task;2020-06-30T17:11:50.000+0000;Blocked;2020-08-20T15:06:27.000+0000;Major - P3;2.0;3;1;0;0;0;19;1;1;5
SERVER;SERVER-49504;allow resmokes mongo shell log jasper allow mongo shell spawned resmoke https github com mongodb mongo blob 316408d14e2358225e8f26bfe4d4022cfcf748d9 buildscripts resmokelib core programs py#l462 log logkeeper v3 suites dont use fixtures run almost entirely shell subprocesses making feature easy way validate new logging path well need set logkeepver v3 related credentials resmoke evergreen yml part ticket well;Task;2020-07-14T21:03:28.000+0000;Open;2020-07-14T21:03:30.000+0000;Major - P3;2.0;1;0;0;0;1;17;1;1;5
SERVER;SERVER-46867;ensure db directory created even alwaysuselogfiles enabled alwaysuselogfiles enabled nocleandata option set make sure previously logged data deleted prevent resetdbpath called however resetdbpath used create db path first place need create directory doesnt exist allowing existing paths cleaned;Bug;2020-03-13T21:10:23.000+0000;In Code Review;2020-08-21T17:01:54.000+0000;Major - P3;2.0;3;1;1;0;1;18;1;1;5
SERVER;SERVER-38589;service mongod stop may produce usr bin mongod found running none killed fix applied https github com mongodb mongo blob 6c8dc5e004bf2c91df10975adef861bcf00af6cd debian init prevent error stopping mongod;Bug;2018-12-13T00:18:59.000+0000;Closed;2019-02-13T15:05:35.000+0000;Major - P3;2.0;4;1;0;0;0;14;1;1;5
SERVER;SERVER-42071;notary client errors system failures cases like https evergreen mongodb com task mongodb_mongo_v4 0_enterprise_rhel_70_64_bit_push_5f93fc9db3a3475dd2c7543b9f1e1179e6f9065f_19_06_14_13_51_46 notary client errors obscured different issue evergreen one line change adding type test line 2437 hard part figuring kind error testred setuplavender want think shouldnt system failure probably make harder diagnose issues;Task;2019-07-03T17:25:59.000+0000;Closed;2020-07-17T17:59:15.000+0000;Major - P3;2.0;3;2;0;0;1;57;1;1;5
SERVER;SERVER-43003;add option archive data files shell spawned processes fail consistency checks data archival hook currently option archive data data consistency checks fail works resmoke spawned processes also pick shell spawned processes consistency checks automatically shutdown;Improvement;2019-08-23T14:53:26.000+0000;Open;2019-08-23T14:53:30.000+0000;Major - P3;2.0;1;1;0;0;1;10;1;1;5
SERVER;SERVER-44632;platform support remove community zseries 4 2 following mongodb 4 4 ga removing community zseries support mongodb 4 2 ssl rhel 6 7 s390x ssl rhel 7 2 s390x ssl sles 12 s390x ssl sles 15 s390x ssl ubuntu 18 04 s390x;Task;2019-11-14T16:43:12.000+0000;Open;2019-11-14T16:43:16.000+0000;Major - P3;2.0;1;0;0;0;0;25;1;1;5
SERVER;SERVER-45074;commit queue commit message validation double check ticket id ticket key real pain make commit wrong ticket number prefix end needing bunch manual work reconcile various places things got logged incorrectly paper mistake since adding validation commit messages ways formatting etc see would possible also validate jira ticket nominally committing makes sense open assigned etc;Improvement;2019-12-11T22:14:18.000+0000;Closed;2020-01-24T17:47:32.000+0000;Major - P3;2.0;8;7;2;0;1;19;1;1;5
SERVER;SERVER-46326;update multiversion tests following 4 4 0 rc0 please take look comments inhttps jira mongodb org browse server 41227that link 5 commits made 4 2 encompass work thats needed done ac update binversion server js https github com mongodb mongo blob 6d58b7371e1bec5b4281f25ea797cdb50bf20e62 src mongo shell servers js#l207 4 4 update binary version constants defined multiversionconstants py https github com mongodb mongo blob 6d58b7371e1bec5b4281f25ea797cdb50bf20e62 buildscripts resmokelib multiversionconstants py#l3 fcv constants file left work done part ofserver 46323 ensure new redness introduced enterprise rhel 6 2 implicit multiversion build variant;Task;2020-02-21T21:30:37.000+0000;Closed;2020-04-16T14:33:50.000+0000;Critical - P2;2.0;17;10;12;0;1;51;2;1;2
SERVER;SERVER-49333;use host create set remote instance powercycle powercycle task launches remote instance powertest https github com mongodb mongo blob f39585101d93f47c216ea8c30e276ac0410c30a2 pytests powertest py run powercycle events fix issues connecting remote instance windows use host create https github com evergreen ci evergreen wiki project commands#host create launch instance;Improvement;2020-07-07T16:09:41.000+0000;In Progress;2020-07-10T17:49:44.000+0000;Major - P3;3.0;4;5;1;0;1;23;1;2;6
SERVER;SERVER-50134;run microbenchmarks tests dsi depends tig 2471# move start server logic mongodb_setup script# use workload_setup test_control genny# use test_control run perf tests# dont write overrides yml use expansions yml call bootstrap # dont require special analysis yml file# call analysis via run dsi;New Feature;2020-08-05T21:50:55.000+0000;Open;2020-08-05T21:50:56.000+0000;Major - P3;3.0;0;0;0;1;1;7;1;2;6
SERVER;SERVER-48521;build store grpcio arm64 ppc s390x pre built wheels grpcio arm64 armv8 ppc s390x cant compiled toolchain box would take 15+ min even compile work need either set ci project compile ask upstream get us version works platforms;New Feature;2020-06-01T18:33:45.000+0000;Blocked;2020-07-27T15:20:25.000+0000;Major - P3;4.0;2;0;0;0;1;51;1;2;6
SERVER;SERVER-48299;provide detail error stopping mongod test errors example message + stack noformat js_testbackup_restore_stop_start 2020 05 18t213552 494+0000 2020 05 18t213552 398z 22821 js shell stopped mongo program port port attr port20021 js_testbackup_restore_stop_start 2020 05 18t213552 948+0000 uncaught exception stoperror mongodb process stopped exit code 9 js_testbackup_restore_stop_start 2020 05 18t213552 948+0000 stoperror mongodb process stopped exit code 9 js_testbackup_restore_stop_start 2020 05 18t213552 948+0000 mongorunner stoperror src mongo shell servers js92444 js_testbackup_restore_stop_start 2020 05 18t213552 948+0000 mongorunner stopmongod src mongo shell servers js102815 js_testbackup_restore_stop_start 2020 05 18t213552 948+0000 replsettest stop src mongo shell replsettest js298419 js_testbackup_restore_stop_start 2020 05 18t213552 948+0000 _nodeparamtoconn src mongo shell replsettest js22657 js_testbackup_restore_stop_start 2020 05 18t213552 948+0000 _nodeparamtosinglenode src mongo shell replsettest js26045 noformat port often thats needed identify node question things tricky port used across restarts information use identify process lifetimes included available process id dbpath used binary name e g mongo mongos mongobridge mongod mongod 4 4;Improvement;2020-05-19T18:25:18.000+0000;Open;2020-05-21T19:10:43.000+0000;Major - P3;3.0;2;0;0;0;1;16;1;2;6
SERVER;SERVER-47621;parse_options powertest py implicitly deduplicates mongod options powercycle evergreen tests make invocation powertest py following mongod options codejava setparameter enabletestcommands1 setparameter logcomponentverbosity storage recovery2 storageengine wiredtiger code one checks mongod log afterwards appears setparameter options deduplicated setparameter logcomponentverbosity storage recovery2 remains seems bug parse_options powertest py creates defaultdict mongod options implicity deduplicates two setparameter options;Bug;2020-04-17T13:49:06.000+0000;Open;2020-05-07T15:38:07.000+0000;Major - P3;3.0;3;0;0;1;1;13;1;2;6
SERVER;SERVER-38817;use generate tasks resmoke tasks migrate resmoke tasks using generate tasks use generate tasks;Improvement;2019-01-03T14:26:42.000+0000;Closed;2020-03-18T21:58:28.000+0000;Major - P3;3.0;4;3;1;0;0;64;1;2;6
SERVER;SERVER-39362;make parallel basic js test respect resmoke py tag based exclusions jstests parallel basic js tests couple shortcomings due javascript test runs javascript tests# resmoke py doesnt know individual tests run therefore cannot apply tag based exclusion using exclude_any_with_tags # resmoke py doesnt spawn separate mongo shell processes individual tests run therefore cannot create separate log endpoints output b report separate pass fail statuses ticket intended address #1 addressing #2a difficult due existing logkeeper schema makes assumption test ended soon another test thats part build_id started addressing #2b difficult creating new test_id tied resmoke py starting test https github com mongodb mongo blob r4 1 7 buildscripts resmokelib testing report py#l92 l117 new parallel_js_test test kind introduced makes use resmoke pys buildscripts resmokelib selector py module within paralleljstestcase class filter tests jstests core directory shouldnt run jstests parallel basic js tests paralleljstestcase _make_process spawn mongo shell process new testdata testschedule array option similar name element corresponds list tests scopedthread spawned jstests parallel basic js tests run codejavascript testdata testschedule list sourced exclusively executor config serial_execution section 0 jstests core killop_drop_collection js jstests core fsync js jstests core currentop js list following ones sourced exclusively tests filted executor config selector section arent present executor config serial_execution section 1 jstests core js 2 jstests core all2 js 3 jstests core all3 js code logic paralleltester createjstestslists function https github com mongodb mongo blob r4 1 7 jstests libs paralleltester js#l143 l323 expressed resmoke py yaml suite file performed paralleljstestcase class particular tests https github com mongodb mongo blob r4 1 7 jstests libs paralleltester js#l212 l222 automatically excluded resmoke py invoked excludewithanytagsrequires_find_command parallel_compatibility evergreen task https github com mongodb mongo blob r4 1 7 etc evergreen yml#l5883 configures order tests testdata testschedule including mentioned executor config serial_execution section shuffled error explicitly mention test executor config selector section doesnt exist happen automatically error explicitly mention test executor config serial_execution section doesnt exist notes number array elements generate testdata testschedule defined constant 4 paralleljstestcase class need configurable via yaml new parallel_jscore_passthrough parallel_jscore_compatibility_passthrough evergreen tasks introduced build variants currently run parallel parallel_compatibility tasks respectively codeyaml titlebuildscripts resmokeconfig suites parallel_jscore_passthrough yml test_kind parallel_js_testselector roots jstests parallel basic js executor archive hooks validatecollections config selector roots jstests core js exclude_files # transactions supported mongodb standalone nodes jstests core txns js serial_execution # following tests run fsync 1 lock 1 command jstests core currentop js jstests core fsync js jstests core killop_drop_collection js shell_options readmode commands hooks class validatecollections fixture class mongodfixture mongod_options set_parameters enabletestcommands 1 code;New Feature;2019-02-04T03:16:37.000+0000;Open;2019-02-04T03:16:42.000+0000;Major - P3;4.0;5;1;1;0;1;4;1;2;6
SERVER;SERVER-39957;two phase drop rename delay second phase drop optime checkpointed majority committed currently old two phase drop rename executes second phase drop wt table file majority commit point moves past drop optime majority commit point ahead checkpoint crash happens second phase drop restart server find metadata collection still catalog loads last checkpoint actual wt file gets dropped restart server detect unclean shutdown examining mongod lock file safely remove metadata collections wt table files however instead crashing second phase drop opening backup cursor would cause similar issue harder solve also inconsistency wt table files catalog since dont copy mongod lock backup server trigger code reconciles catalog tries open wt file exist hit fassert https github com mongodb mongo blob 6f3c3df4fc0abda76fd97e970ced4a01f0c48007 src mongo db storage wiredtiger wiredtiger_record_store cpp#l666 fix problem delay second phase drop optime checkpointed;Bug;2019-03-05T00:41:47.000+0000;Closed;2019-03-12T19:23:24.000+0000;Major - P3;3.0;7;2;0;0;0;59;1;2;6
SERVER;SERVER-40433;mongodump extremely slow 4 0 8 since upgrading mongo 4 0 8 3 4 6 mongodumps become extremely slow mongodump archive dir gz h mongo_repl_set_name mongo_set u mongo_user p mongo_pwd gzip readpreference secondarypreferred ssl sslcafile mongodb ca crt authenticationdatabase admin db_or_oplog forcetablescanis command run backups since upgrade seeing 10x slower backups tried moving new machine memory didnt see speed improvement cpu 1 core 100 rest cores little activity iostat doesnt show disk anywhere near saturated tried running mongodump 3 6 4 0 cli without improvement;Bug;2019-04-02T00:54:36.000+0000;Closed;2019-08-20T14:34:00.000+0000;Major - P3;3.0;21;36;4;1;0;58;1;2;6
SERVER;SERVER-40433;mongodump extremely slow 4 0 8 since upgrading mongo 4 0 8 3 4 6 mongodumps become extremely slow mongodump archive dir gz h mongo_repl_set_name mongo_set u mongo_user p mongo_pwd gzip readpreference secondarypreferred ssl sslcafile mongodb ca crt authenticationdatabase admin db_or_oplog forcetablescanis command run backups since upgrade seeing 10x slower backups tried moving new machine memory didnt see speed improvement cpu 1 core 100 rest cores little activity iostat doesnt show disk anywhere near saturated tried running mongodump 3 6 4 0 cli without improvement;Bug;2019-04-02T00:54:36.000+0000;Closed;2019-06-24T00:53:26.000+0000;Major - P3;3.0;21;36;4;1;0;58;1;2;6
SERVER;SERVER-41048;address regression transaction performance storage stats collection appx 15 regression transaction performance due collection storage statistics individual operation transaction ticket tracks work work required resolve issue;Improvement;2019-05-08T07:51:58.000+0000;Open;2019-05-23T23:34:42.000+0000;Major - P3;3.0;5;6;1;0;1;20;1;2;6
SERVER;SERVER-42140;de duplicate generate_compile_expansions build script following two files largely duplicated makes complicated edit maintain lead complications release branching processes around 4 2 files generate_compile_expansions pygenerate_compile_expansions_shared_cache pyi think important backport v4 2;Task;2019-07-10T15:57:50.000+0000;Open;2019-10-03T16:57:53.000+0000;Major - P3;3.0;1;0;0;0;1;34;1;2;6
SERVER;SERVER-46941;add suite mode burn_in_tags add mode burn_in_tag detect changes test suites run entire suite specified build variants server engineer want changed test suites detected run burn_in_tags feel confident changes work non required build variants ac changes test suites cause entire suite run specified build variants burn_in_tags new test suites run specified build variants burn_in_tags see project scope https docs google com document 1 syoxkhm9sryhpxfcnuxedrcbfkmvb82vmdeqryuppg edit#headingh sye21yee6q34;Task;2020-03-18T14:18:20.000+0000;Open;2020-03-18T14:18:21.000+0000;Major - P3;3.0;1;0;1;0;1;18;1;2;6
SERVER;SERVER-39957;two phase drop rename delay second phase drop optime checkpointed majority committed currently old two phase drop rename executes second phase drop wt table file majority commit point moves past drop optime majority commit point ahead checkpoint crash happens second phase drop restart server find metadata collection still catalog loads last checkpoint actual wt file gets dropped restart server detect unclean shutdown examining mongod lock file safely remove metadata collections wt table files however instead crashing second phase drop opening backup cursor would cause similar issue harder solve also inconsistency wt table files catalog since dont copy mongod lock backup server trigger code reconciles catalog tries open wt file exist hit fassert https github com mongodb mongo blob 6f3c3df4fc0abda76fd97e970ced4a01f0c48007 src mongo db storage wiredtiger wiredtiger_record_store cpp#l666 fix problem delay second phase drop optime checkpointed;Bug;2019-03-05T00:41:47.000+0000;Closed;2020-04-02T20:34:16.000+0000;Major - P3;3.0;7;2;0;0;0;59;1;2;6
SERVER;SERVER-21861;better timestamp object comparison mongo shell would great operators timestamp objects would work expected bsonwocompare used mean time even though end comparing member functions well long fields compared first fields;Improvement;2015-12-11T15:35:39.000+0000;Closed;2020-08-05T02:00:07.000+0000;Major - P3;3.0;17;16;5;1;1;52;1;2;6
SERVER;SERVER-35109;enable signal processing perf yml run command signal processing analyze phase task;Task;2018-05-21T13:26:48.000+0000;Closed;2018-10-02T15:47:03.000+0000;Major - P3;3.0;4;5;0;0;1;52;1;2;6
SERVER;SERVER-36997;stepdown thread perform wall write stepping restarting node stepdown thread steps node nodes set may go rollback rollback happen unpredictable times since based node gets new batch triggers oplogstartmissing error rollback occurs node closes connections stepdown thread robust connections closing arbitrary points stepdown thread wait rollbacks complete performing wall write stepping node;Bug;2018-09-05T15:54:51.000+0000;Open;2018-09-08T20:26:18.000+0000;Major - P3;3.0;3;2;1;0;1;70;1;2;6
SERVER;SERVER-37465;remove rosette linguistics platform options debian files per discussion https jira mongodb org browse pm 1178pagecom atlassian jira plugin system issuetabpanels3acomment tabpanelfocusedcommentid2012178#comment 2012178 references rosette linguistics platform remain debian files need removed;Task;2018-10-03T20:50:54.000+0000;Open;2019-01-04T19:03:18.000+0000;Major - P3;3.0;8;6;0;0;1;53;1;2;6
SERVER;SERVER-37769;platform support add community enterprise sles 15 x64 platform support add community enterprise sles 15 x64;Task;2018-10-26T16:36:39.000+0000;Closed;2019-04-29T17:40:33.000+0000;Major - P3;3.0;9;6;3;0;1;51;1;2;6
SERVER;SERVER-37772;platform support add community enterprise rhel 8 x64 platform support add community enterprise rhel 8 x64;Task;2018-10-26T16:37:05.000+0000;Closed;2019-07-29T17:48:23.000+0000;Major - P3;3.0;18;9;3;0;5;57;1;2;6
SERVER;SERVER-37776;platform support add community enterprise sles15 zseries s390x platform support add community enterprise sles15 zseries;Task;2018-10-26T16:38:09.000+0000;Open;2019-07-15T13:34:43.000+0000;Major - P3;3.0;4;0;0;0;1;68;1;2;6
SERVER;SERVER-37776;platform support add community enterprise sles15 zseries s390x platform support add community enterprise sles15 zseries;Task;2018-10-26T16:38:09.000+0000;Open;2018-10-26T16:38:10.000+0000;Major - P3;3.0;4;0;0;0;1;68;1;2;6
SERVER;SERVER-48111;format python black kill yapf favor black replace yapflinter class blacklinter disable change pylint checks fail result consider relaxing number pydocstyle checks tend result useless comments added appease linter d103 missing docstring public function d107 missing docstring __init__ d100 missing docstring public module d101 missing docstring public class d200 one line docstring fit one line quotes;Improvement;2020-05-11T21:03:32.000+0000;Open;2020-05-11T21:03:33.000+0000;Major - P3;3.0;3;1;0;0;1;5;1;2;6
SERVER;SERVER-29446;sample stage could find non duplicate document using random cursor error originally reported server 20385 marked fixed 3 1 9 still able reproduce version 3 4 4;Bug;2017-06-05T15:38:57.000+0000;Closed;2019-08-16T01:09:28.000+0000;Major - P3;8.0;23;14;3;2;0;127;1;2;6
SERVER;SERVER-28940;make resmoke fixture setup teardown testcases fixture setup teardown logs go logkeeper dont show evergreen sidebar assertions cant extracted without figuring logkeeper url task log since everything else logs logkeeper e test hooks testcase shows sidebar fixture events get spots well;Improvement;2017-04-24T15:21:25.000+0000;Closed;2018-10-04T13:52:24.000+0000;Major - P3;5.0;4;9;4;1;1;51;1;2;6
SERVER;SERVER-26988;secondary delay causes large drop insert rate primary due cache full condition heavy insert load secondary delayed cache primary fills 100 operation rates drop heres run showing behavior primary secondary delayed due lag similar effect seen secondary intentionally delayed using slavedelay lagging png width100 e cache 95 full insert rate drops considerably possibly due application threads evictions f g h seem seem related checkpoints possibly also combination full cache rate pages walked eviction generally high 6k times rate pages actually evicted suggesting issue difficulty finding pages evict keep cache target levelsthe high rate pages walked eviction suggests connection server 22831 also showed symptom connection full cache however run 3 2 5 rc1 server 22831 fixed seems different issue test involved 3 2 5 rc1 2 node replica set 25 gb cache 100 gb oplog 5 threads inserting 800 byte documents 5 separate collections code seq 5 mongo eval x var i0 i800 i++ x + x docs var i0 i1000 i++ docs push xx ops op insert ns test c + doc docs res benchrun ops ops seconds 10000 parallel 1 donewait code;Bug;2016-11-10T21:58:01.000+0000;Closed;2019-05-02T01:53:54.000+0000;Major - P3;5.0;15;3;2;0;0;67;1;2;6
SERVER;SERVER-16612;implicitly zeroed files wiredtiger problem implicit zeroing files kernel certain platforms see server 15369 details workaround put place issue mmapv1 files explicitly zero ns files purpose ticket determine areas wiredtiger similar vulnerabilities issue customer impact issue wiredtiger beyond advising customers avoid using wiredtiger platforms issue reasonably work around problem explicitly zeroing files rather relying kernels implicit zeroing;Task;2014-12-19T20:33:42.000+0000;Closed;2019-05-15T19:57:50.000+0000;Major - P3;5.0;16;3;1;0;0;53;1;2;6
SERVER;SERVER-34598;add millisecond granularity wallclock times various metrics replsetgetstatuss optimes subdocument response replsetgetstatus includes subdocument named optimes contains optime various important oplog events including lastcommittedoptime readconcernmajorityoptime appliedoptime durableoptime mongodb 3 6 actual oplog entries corresponding optimes wall clock time milliseconds resolution recorded extend replsetgetstatus report wall clock times corresponding optimes usually get millisecond granularity measurements replication lag back back majority read modify write latencies work ticket split server 40080 server 40078 server 40353 server 34598 umbrella ticket work items;Improvement;2018-04-20T19:46:13.000+0000;Closed;2019-04-30T20:28:15.000+0000;Major - P3;5.0;15;10;7;0;0;78;1;2;6
SERVER;SERVER-37644;make createindexes command join already progress index builds depends server 37643 move index builds behind index build interface established server 37636 createindexes command check whether index es already built wait upon new waiting function must added index build interface appropriate error message returned commitquorum match progress index build indexes specs match identically single index builder note multiple indexes key pattern different collations https github com mongodb mongo blob 9f363b489585124afa1e26412e19f6728763e1ad src mongo db catalog index_catalog_impl cpp#l749 l768 server 24239;Task;2018-10-15T20:55:32.000+0000;Closed;2020-02-26T19:30:09.000+0000;Major - P3;5.0;4;5;4;0;0;107;1;2;6
SERVER;SERVER-38562;implement indexbuildscoordinatorvotecommitindexbuilds consider moving clientsetlastoptosystemlastoptime https github com mongodb mongo blob 597b4748fc36210c61cf4d6c086d364013df740a src mongo db commands vote_commit_index_builds_command cpp#l77 l80 logic function logic flag must set memory index build whether proceed without voting end thread voting successfully finding flag set fact commitindexbuild build already reached committing phase set flag start new asynchronous thread commit else memory flag set index build discovers later bypasses voting proceeding straight commit necessary stalling commit thereby stalling replication secondary cannot permitted take long network call potentially take matter seconds presumably alternative would make votecommitindexbuild command sent secondary short enough timeout dont mind stalling replication amount time risky given determining reasonable time network replication latencies might impossible index build thread would exist finishing voting opposed waiting condition variable commitindexbuild signal replindexbuildstate currently expecting condition variable already set waiting used need change flag must also initialized correctly index build recovery depending persisted state;Task;2018-12-12T14:57:45.000+0000;Closed;2020-02-27T16:22:06.000+0000;Major - P3;5.0;2;2;3;0;0;80;1;2;6
SERVER;SERVER-49096;replica set tests log pid port topology goal make convenient find mappings without look invocation process also include ports used mongobridge investigate whether log output resmoke rotates logs test;Improvement;2020-06-25T14:28:33.000+0000;Open;2020-07-23T15:09:44.000+0000;Major - P3;5.0;3;0;1;0;1;14;1;2;6
SERVER;SERVER-48909;fix bug sharded backup restore test loses record mongodb test suite seen failure sharded backup restore test doesnt see expected set records end test test run majority read concern test case code src mongo db modules enterprise jstests hot_backups sharded_last_stable_pit_backup_restore_simple js code stacktrace noformat assert src mongo shell assert js1519_verifydataiscausallyconsistent src mongo db modules enterprise jstests hot_backups libs sharded_backup_restore js15913_checkdataconsistency src mongo db modules enterprise jstests hot_backups libs sharded_backup_restore js21727shardedbackuprestoretest run src mongo db modules enterprise jstests hot_backups libs sharded_backup_restore js9609 src mongo db modules enterprise jstests hot_backups sharded_last_stable_pit_backup_restore_simple js2411 src mongo db modules enterprise jstests hot_backups sharded_last_stable_pit_backup_restore_simple js102 noformat logs noformat assert failed doc 100 missing noformat;Bug;2020-06-11T23:25:44.000+0000;Blocked;2020-07-08T22:14:48.000+0000;Major - P3;8.0;12;11;1;0;1;26;1;2;6
SERVER;SERVER-46940;add suite mode burn_in_tests add mode burn_in_tests detect changes test suites run entire suite multiple times server engineer want changed test suites detected run burn_in_tests feel confident changes introduce reliability issues ac changes test suites cause entire suite run multiple times burn_in_tests new test suites run multiple times burn_in_tests see project scope https docs google com document 1 syoxkhm9sryhpxfcnuxedrcbfkmvb82vmdeqryuppg edit#headingh sye21yee6q34;Task;2020-03-18T14:15:51.000+0000;Open;2020-03-25T19:18:19.000+0000;Major - P3;5.0;1;1;1;0;1;31;1;2;6
SERVER;SERVER-46940;add suite mode burn_in_tests add mode burn_in_tests detect changes test suites run entire suite multiple times server engineer want changed test suites detected run burn_in_tests feel confident changes introduce reliability issues ac changes test suites cause entire suite run multiple times burn_in_tests new test suites run multiple times burn_in_tests see project scope https docs google com document 1 syoxkhm9sryhpxfcnuxedrcbfkmvb82vmdeqryuppg edit#headingh sye21yee6q34;Task;2020-03-18T14:15:51.000+0000;Open;2020-04-23T14:11:15.000+0000;Major - P3;5.0;1;1;1;0;1;31;1;2;6
SERVER;SERVER-46940;add suite mode burn_in_tests add mode burn_in_tests detect changes test suites run entire suite multiple times server engineer want changed test suites detected run burn_in_tests feel confident changes introduce reliability issues ac changes test suites cause entire suite run multiple times burn_in_tests new test suites run multiple times burn_in_tests see project scope https docs google com document 1 syoxkhm9sryhpxfcnuxedrcbfkmvb82vmdeqryuppg edit#headingh sye21yee6q34;Task;2020-03-18T14:15:51.000+0000;Open;2020-04-30T17:49:52.000+0000;Major - P3;5.0;1;1;1;0;1;31;1;2;6
SERVER;SERVER-39705;indexbuildinterceptor faithfully preserve multikey document generates keys indexbuildinterceptor makes incorrect assumption document must generate keys https github com mongodb mongo blob 04882fa7f5210cfb14918ecddbbc5acbd88e86b6 src mongo db index index_build_interceptor cpp#l384 l386 considered multikey https github com mongodb mongo blob 04882fa7f5210cfb14918ecddbbc5acbd88e86b6 src mongo db index index_build_interceptor cpp#l389 l397 particular sparse compound indexes https docs mongodb com manual core index sparse #sparse compound indexes may generate keys consider document multikey 1 mongodbs validation code strict compare indexs multikey multikey output every document https github com mongodb mongo blob 04882fa7f5210cfb14918ecddbbc5acbd88e86b6 src mongo db catalog private record_store_validate_adaptor cpp#l110 l117 1 consider index 1 b 2dsphere 2dsphere makes index auto sparse consider document _id 1 1 2 b omitted sparse ness result index keys generated however array field compound index considered multikey;Bug;2019-02-21T02:44:36.000+0000;Closed;2019-02-21T13:37:51.000+0000;Major - P3;8.0;10;23;2;0;2;56;1;2;6
SERVER;SERVER-39705;indexbuildinterceptor faithfully preserve multikey document generates keys indexbuildinterceptor makes incorrect assumption document must generate keys https github com mongodb mongo blob 04882fa7f5210cfb14918ecddbbc5acbd88e86b6 src mongo db index index_build_interceptor cpp#l384 l386 considered multikey https github com mongodb mongo blob 04882fa7f5210cfb14918ecddbbc5acbd88e86b6 src mongo db index index_build_interceptor cpp#l389 l397 particular sparse compound indexes https docs mongodb com manual core index sparse #sparse compound indexes may generate keys consider document multikey 1 mongodbs validation code strict compare indexs multikey multikey output every document https github com mongodb mongo blob 04882fa7f5210cfb14918ecddbbc5acbd88e86b6 src mongo db catalog private record_store_validate_adaptor cpp#l110 l117 1 consider index 1 b 2dsphere 2dsphere makes index auto sparse consider document _id 1 1 2 b omitted sparse ness result index keys generated however array field compound index considered multikey;Bug;2019-02-21T02:44:36.000+0000;Closed;2019-06-11T19:19:47.000+0000;Major - P3;8.0;10;23;2;0;2;56;1;2;6
SERVER;SERVER-39705;indexbuildinterceptor faithfully preserve multikey document generates keys indexbuildinterceptor makes incorrect assumption document must generate keys https github com mongodb mongo blob 04882fa7f5210cfb14918ecddbbc5acbd88e86b6 src mongo db index index_build_interceptor cpp#l384 l386 considered multikey https github com mongodb mongo blob 04882fa7f5210cfb14918ecddbbc5acbd88e86b6 src mongo db index index_build_interceptor cpp#l389 l397 particular sparse compound indexes https docs mongodb com manual core index sparse #sparse compound indexes may generate keys consider document multikey 1 mongodbs validation code strict compare indexs multikey multikey output every document https github com mongodb mongo blob 04882fa7f5210cfb14918ecddbbc5acbd88e86b6 src mongo db catalog private record_store_validate_adaptor cpp#l110 l117 1 consider index 1 b 2dsphere 2dsphere makes index auto sparse consider document _id 1 1 2 b omitted sparse ness result index keys generated however array field compound index considered multikey;Bug;2019-02-21T02:44:36.000+0000;Closed;2019-06-16T10:48:56.000+0000;Major - P3;8.0;10;23;2;0;2;56;1;2;6
SERVER;SERVER-38396;improve indexbuildscoordinator unit testing made interface two implementations server 38323 added embedded implementation original unit test tests original implementation task cover implementations unit testing;Task;2018-12-04T19:15:29.000+0000;Closed;2020-03-16T20:52:09.000+0000;Major - P3;8.0;0;0;1;0;0;57;1;2;6
SERVER;SERVER-39458;add continuous draining secondarys index build thread awaits commitindexbuild oplog entry secondary oplog application indexbuildscoordinator periodically drain side tables awaiting commitindexbuild abortindexbuild oplog entries primary;Task;2019-02-08T16:09:36.000+0000;Closed;2020-04-03T14:08:05.000+0000;Major - P3;8.0;4;5;3;0;2;91;1;2;6
SERVER;SERVER-39451;add recover stable timestamp logic startindexbuild abortindexbuild commitindexbuild entering rollback abort active index builds https github com mongodb mongo blob 6abbac58cc5b5f4b66b50ada20e70fdf96301571 src mongo db repl bgsync cpp#l635 rolling back add logic reconcilecatalogandidents https github com mongodb mongo blob c046a5896652acea84c9db1d9346a43b2745a37b src mongo db storage storage_engine_impl cpp#l324 restart unfinished two phase builds;Task;2019-02-08T14:20:32.000+0000;Closed;2019-11-14T16:27:40.000+0000;Major - P3;8.0;2;3;11;0;1;77;1;2;6
SERVER;SERVER-39428;record indexing errors simultaneous index builds later constraint checking hybrid index builds record duplicate key conflicts side table later resolution simultaneous index builds primary secondary need record conflicts case secondary becomes primary becomes responsible constraint checking today secondaries also ignore types indexing errors maintain idempotency guarantee errors resolved primary cannot send createindexes oplog entry unless simultaneous indexes secondaries cannot ignore indexing errors must also record conflicts side table secondary becomes primary needs guarantee indexing errors resolved;Improvement;2019-02-07T22:27:53.000+0000;Closed;2020-01-23T20:30:52.000+0000;Major - P3;8.0;3;0;2;0;0;70;1;2;6
SERVER;SERVER-39239;two phase index builds secondaries wait commitindexbuild oplog entry committing secondarys index build thread spin loop finishing committing index secondary nothing spinning loop drop locks server 39458 add functionality loop periodically reacquiring lock running side table draining receipt commitindexbuild oplog entry secondary signal index build commit timestamp passed oplog entry oplog applier thread need drop locks allow index build take x lock index commit believe safe since c oplog entries applied serially dropping lock ok indexbuildscoordinatorcommitindexbuild need fetch future index build return oplog applier thread wait upon commit completion server 39533 done ticket hook abortindexbuild oplog entry alternatively index build secondary aborted spinning loop waiting commit case thats design consideration try act abort commit signals earlier index build secondaries spinning loop waits commit abort simple first iteration implementation well make fancier subsequent ticket;Task;2019-01-28T22:25:50.000+0000;Closed;2019-09-23T14:49:03.000+0000;Major - P3;5.0;3;9;20;0;1;88;1;2;6
SERVER;SERVER-39219;insert performance mongodb 3 4 10 mongodb 4 0 5 mongostat insert700 million data mongodb 3 4 10 needs 4 5 hours upgrade mongodb 4 0 5 needs 6 5 hours time goes insert data need time check setting improve performance mongodb 3 4 10 cache dirty 6 mongodb 4 0 5 cache dirty 20 install mongodb default setting disable numa related content attachments thanks lot;Question;2019-01-28T09:42:50.000+0000;Open;2019-04-24T00:01:57.000+0000;Major - P3;5.0;14;20;0;1;1;47;1;2;6
SERVER;SERVER-39219;insert performance mongodb 3 4 10 mongodb 4 0 5 mongostat insert700 million data mongodb 3 4 10 needs 4 5 hours upgrade mongodb 4 0 5 needs 6 5 hours time goes insert data need time check setting improve performance mongodb 3 4 10 cache dirty 6 mongodb 4 0 5 cache dirty 20 install mongodb default setting disable numa related content attachments thanks lot;Question;2019-01-28T09:42:50.000+0000;Open;2019-05-07T15:19:13.000+0000;Major - P3;5.0;14;20;0;1;1;47;1;2;6
SERVER;SERVER-39219;insert performance mongodb 3 4 10 mongodb 4 0 5 mongostat insert700 million data mongodb 3 4 10 needs 4 5 hours upgrade mongodb 4 0 5 needs 6 5 hours time goes insert data need time check setting improve performance mongodb 3 4 10 cache dirty 6 mongodb 4 0 5 cache dirty 20 install mongodb default setting disable numa related content attachments thanks lot;Question;2019-01-28T09:42:50.000+0000;Open;2019-07-24T23:50:06.000+0000;Major - P3;5.0;14;20;0;1;1;47;1;2;6
SERVER;SERVER-39004;introduce quota mechanism overflow file dont currently quota mechanism prevent wiredtigerlas wt growing eventually running disk space would help configuration place file reaches configured size reboot mongod process effectively clean wiredtigerlas wt file;Improvement;2019-01-15T08:03:33.000+0000;Closed;2019-06-12T02:17:52.000+0000;Major - P3;5.0;27;14;4;2;4;96;1;2;6
SERVER;SERVER-39004;introduce quota mechanism overflow file dont currently quota mechanism prevent wiredtigerlas wt growing eventually running disk space would help configuration place file reaches configured size reboot mongod process effectively clean wiredtigerlas wt file;Improvement;2019-01-15T08:03:33.000+0000;Closed;2019-08-14T00:06:34.000+0000;Major - P3;5.0;27;14;4;2;4;96;1;2;6
SERVER;SERVER-38735;extended stalls cache pressure attached script taken another ticket creates cache pressure primary lagging secondary isnt ideal condition storage engine appears 3 6 significantly better workload 4 0stalls png width100 3 6 workload completed significantly faster 4 0 extended total stalls 80s whereas worst 3 6 tests 8s appears 4 0 2 may little worse 4 0 0 although erratic performance makes data noisy much longer runs would probably needed see real differenceftdc data 9 stack samples taken one stalls attached couple top stacks counts 9 samples one application threads evicting data threads waiting code 109 pthread_cond_timedwait glibc_2 3 2238 __wt_cond_wait_signal __wt_cache_eviction_worker __wt_txn_commit __session_commit_transaction mongowiredtigerrecoveryunit_txnclose mongowiredtigerrecoveryunit_commit mongowriteunitofworkcommit mongo anonymous namespace insertdocuments mongoperforminserts mongo anonymous namespace cmdinsertinvocationrunimpl mongo anonymous namespace writecommandinvocationbaserun mongo anonymous namespace invokeintransaction mongo anonymous namespace execcommanddatabase mongo anonymous namespace receivedcommands mongoserviceentrypointcommonhandlerequest mongoserviceentrypointmongodhandlerequest mongoservicestatemachine_processmessage mongoservicestatemachine_runnextinguard std_function_handler mongotransportserviceexecutorsynchronousschedule mongoservicestatemachine_schedulenextwithguard mongoservicestatemachine_sourcecallback mongoservicestatemachine_sourcemessage mongoservicestatemachine_runnextinguard std_function_handler std_function_handler mongo anonymous namespace runfunc start_thread333 clone109 15 __lll_lock_wait135 __gi___pthread_mutex_lock80 __split_internal_lock __wt_split_multi __wt_evict __evict_page __wt_cache_eviction_worker __wt_txn_commit __session_commit_transaction mongowiredtigerrecoveryunit_txnclose mongowiredtigerrecoveryunit_commit mongowriteunitofworkcommit mongo anonymous namespace insertdocuments mongoperforminserts mongo anonymous namespace cmdinsertinvocationrunimpl mongo anonymous namespace writecommandinvocationbaserun mongo anonymous namespace invokeintransaction mongo anonymous namespace execcommanddatabase mongo anonymous namespace receivedcommands mongoserviceentrypointcommonhandlerequest mongoserviceentrypointmongodhandlerequest mongoservicestatemachine_processmessage mongoservicestatemachine_runnextinguard std_function_handler mongotransportserviceexecutorsynchronousschedule mongoservicestatemachine_schedulenextwithguard mongoservicestatemachine_sourcecallback mongoservicestatemachine_sourcemessage mongoservicestatemachine_runnextinguard std_function_handler std_function_handler mongo anonymous namespace runfunc start_thread333 clone109 9 pwrite6481 __posix_file_write __block_write_off __wt_block_write __wt_bt_write __rec_split_write __wt_reconcile __wt_evict __evict_page __wt_cache_eviction_worker __wt_txn_commit __session_commit_transaction mongowiredtigerrecoveryunit_txnclose mongowiredtigerrecoveryunit_commit mongowriteunitofworkcommit mongo anonymous namespace insertdocuments mongoperforminserts mongo anonymous namespace cmdinsertinvocationrunimpl mongo anonymous namespace writecommandinvocationbaserun mongo anonymous namespace invokeintransaction mongo anonymous namespace execcommanddatabase mongo anonymous namespace receivedcommands mongoserviceentrypointcommonhandlerequest mongoserviceentrypointmongodhandlerequest mongoservicestatemachine_processmessage mongoservicestatemachine_runnextinguard std_function_handler mongotransportserviceexecutorsynchronousschedule mongoservicestatemachine_schedulenextwithguard mongoservicestatemachine_sourcecallback mongoservicestatemachine_sourcemessage mongoservicestatemachine_runnextinguard std_function_handler std_function_handler mongo anonymous namespace runfunc start_thread333 clone109 code;Bug;2018-09-25T15:18:13.000+0000;In Progress;2019-06-06T03:14:01.000+0000;Major - P3;5.0;26;0;0;2;1;69;1;2;6
SERVER;SERVER-39452;add rollback via refetch logic startindexbuild abortindexbuild commitindexbuild read relevant design documents sections details edge case handling;Task;2019-02-08T14:20:57.000+0000;Closed;2019-12-16T18:51:28.000+0000;Major - P3;5.0;4;4;9;0;1;72;1;2;6
SERVER;SERVER-39087;move initial sync index creation logic indexbuildscoordinator index creation initial sync delegated indexbuildscoordinator;Task;2019-01-18T17:57:00.000+0000;Closed;2020-03-03T02:35:13.000+0000;Major - P3;13.0;0;1;2;0;0;62;1;2;6
SERVER;SERVER-37643;add createindexes command logic index build interface index builder interface established established server 37636 ticket add threadpool move instances multiindexblock index builder class place behind interface running threadpool able register index builds via interface wait upon condition variable hear back status result keep mind server 37644 make index builds joinable via createindexes command condition variable setup must multiple waiters hear back result maybe interface internal helper function get something wait upon status result;Task;2018-10-15T20:46:37.000+0000;Closed;2019-02-01T18:30:14.000+0000;Major - P3;20.0;3;16;12;0;1;81;1;2;6
